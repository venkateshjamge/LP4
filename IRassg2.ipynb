{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import networkx as nx \n",
        "from networkx.exception import NetworkXError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rTFmeNeIG0rr"
      },
      "outputs": [],
      "source": [
        "def pagerank(G, alpha=0.85, personalization=None, max_iter=100, tol=1.0e-6, \n",
        "             nstart=None, weight='weight', dangling=None): \n",
        "\n",
        "    if len(G) == 0: \n",
        "        return {} \n",
        "    if not G.is_directed(): \n",
        "        D = G.to_directed() \n",
        "    else: \n",
        "        D = G \n",
        "\n",
        "    # Create a copy in (right) stochastic form \n",
        "    W = nx.stochastic_graph(D, weight=weight) \n",
        "    N = W.number_of_nodes() \n",
        "\n",
        "    # Choose fixed starting vector if not given \n",
        "    if nstart is None: \n",
        "        x = dict.fromkeys(W, 1.0 / N) \n",
        "    else: \n",
        "        # Normalized nstart vector \n",
        "        s = float(sum(nstart.values())) \n",
        "        x = dict((k, v / s) for k, v in nstart.items()) \n",
        "\n",
        "    if personalization is None: \n",
        "        # Assign uniform personalization vector if not given \n",
        "        p = dict.fromkeys(W, 1.0 / N) \n",
        "    else: \n",
        "        missing = set(G) - set(personalization) \n",
        "        if missing: \n",
        "            raise NetworkXError('Personalization dictionary must have a value for every node. '\n",
        "                                ' Missing nodes %s' % missing) \n",
        "        s = float(sum(personalization.values())) \n",
        "        p = dict((k, v / s) for k, v in personalization.items()) \n",
        "\n",
        "    if dangling is None: \n",
        "        # Use personalization vector if dangling vector not specified \n",
        "        dangling_weights = p \n",
        "    else: \n",
        "        missing = set(G) - set(dangling) \n",
        "        if missing: \n",
        "            raise NetworkXError('Dangling node dictionary must have a value for every node. '\n",
        "                                'Missing nodes %s' % missing) \n",
        "        s = float(sum(dangling.values())) \n",
        "        dangling_weights = dict((k, v/s) for k, v in dangling.items()) \n",
        "    dangling_nodes = [n for n in W if W.out_degree(n, weight=weight) == 0.0] \n",
        "\n",
        "    # power iteration: make up to max_iter iterations \n",
        "    for _ in range(max_iter): \n",
        "        xlast = x \n",
        "        x = dict.fromkeys(xlast.keys(), 0) \n",
        "        danglesum = alpha * sum(xlast[n] for n in dangling_nodes) \n",
        "        for n in x: \n",
        "            # this matrix multiply looks odd because it is \n",
        "            # doing a left multiply x^T=xlast^T*W \n",
        "            for nbr in W[n]: \n",
        "                x[nbr] += alpha * xlast[n] * W[n][nbr][weight] \n",
        "            x[n] += danglesum * dangling_weights[n] + (1.0 - alpha) * p[n] \n",
        "        # check convergence, l1 norm \n",
        "        err = sum([abs(x[n] - xlast[n]) for n in x]) \n",
        "        if err < N*tol: \n",
        "            return x \n",
        "\n",
        "    raise NetworkXError('pagerank: power iteration failed to converge '\n",
        "                        'in %d iterations.' % max_iter) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xq2-4PnbG5Nh",
        "outputId": "ce5295ac-294f-4c07-a0d2-a9ddc47b1341"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: 0.02798909453932378,\n",
              " 1: 0.013176226490010075,\n",
              " 2: 0.012543974790032827,\n",
              " 3: 0.0125633665059297,\n",
              " 4: 0.01337673141185962,\n",
              " 5: 0.013381471466771367,\n",
              " 6: 0.013575814762609998,\n",
              " 7: 0.012971721129320195,\n",
              " 8: 0.01274922256130476,\n",
              " 9: 0.01277666584092255,\n",
              " 10: 0.012364904708174442,\n",
              " 11: 0.013155623246336168,\n",
              " 12: 0.012774526201284197,\n",
              " 13: 0.012950897461788763,\n",
              " 14: 0.012572623732894538,\n",
              " 15: 0.013182689995143985,\n",
              " 16: 0.01275720283292269,\n",
              " 17: 0.01297425762142516,\n",
              " 18: 0.013371088978062592,\n",
              " 19: 0.01357862928365683,\n",
              " 20: 0.01275773107965887,\n",
              " 21: 0.013369487491192794,\n",
              " 22: 0.012569610577560392,\n",
              " 23: 0.012547807194234274,\n",
              " 24: 0.013168284714900124,\n",
              " 25: 0.01337798260548712,\n",
              " 26: 0.012567895279522519,\n",
              " 27: 0.012974370639014111,\n",
              " 28: 0.013161204286829794,\n",
              " 29: 0.013152894414681171,\n",
              " 30: 0.012975757065503683,\n",
              " 31: 0.012383730022267003,\n",
              " 32: 0.012363023679373426,\n",
              " 33: 0.012968818651035535,\n",
              " 34: 0.012989013440387946,\n",
              " 35: 0.01274362176584864,\n",
              " 36: 0.013175578571822917,\n",
              " 37: 0.012556907639510303,\n",
              " 38: 0.012566340456707533,\n",
              " 39: 0.012766676935927751,\n",
              " 40: 0.013566193809930474,\n",
              " 41: 0.013384883444969149,\n",
              " 42: 0.027642388905894218,\n",
              " 43: 0.02723590582442472,\n",
              " 44: 0.026946276210356175,\n",
              " 45: 0.026454497043588543,\n",
              " 46: 0.02613855081939709,\n",
              " 47: 0.025717233201196954,\n",
              " 48: 0.025344973553041043,\n",
              " 49: 0.0250343604362653,\n",
              " 50: 0.02462895045499197,\n",
              " 51: 0.02440861831164504,\n",
              " 52: 0.024033602298471617,\n",
              " 53: 0.023547006390130788,\n",
              " 54: 0.023229012559632736,\n",
              " 55: 0.02284821292651234,\n",
              " 56: 0.022640670934281563,\n",
              " 57: 0.021783817953267025,\n",
              " 58: 0.021687991970735947,\n",
              " 59: 0.02180338288002729}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "G=nx.barabasi_albert_graph(60,41) \n",
        "pr=nx.pagerank(G,0.4) \n",
        "pr "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.1 32-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "ebabc44c36a170a15db727713ef26e039d3379bc758346fdc239d934112a086c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
